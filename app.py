import streamlit as st
import pandas as pd
import numpy as np

from scraper import scrape_site_articles
from analyzer import (
    compute_similarity_report,
    build_similarity_matrix,
)

st.set_page_config(
    page_title="SEO Cannibalization Auditor",
    layout="wide",
)

st.title("SEO Cannibalization Auditor (Scrape ‚Üí Markdown CSV ‚Üí Similarity)")

with st.expander("Uwaga / dobre praktyki", expanded=False):
    st.markdown(
        """
- Narzƒôdzie pobiera tre≈õci ze stron WWW. Upewnij siƒô, ≈ºe masz prawo do scrapowania analizowanej witryny.
- W praktyce warto respektowaƒá robots.txt i nie przeciƒÖ≈ºaƒá serwera (rate limiting jest wbudowany).
- Wyniki podobie≈Ñstwa to sygna≈Ç do audytu ‚Äî nie zawsze oznaczajƒÖ realnƒÖ kanibalizacjƒô.
        """.strip()
    )

colA, colB, colC = st.columns([2, 1, 1])

with colA:
    base_url = st.text_input(
        "Adres strony (np. https://hipoteczny.pl)",
        value="https://hipoteczny.pl",
        placeholder="https://example.com",
    )

with colB:
    max_pages = st.number_input(
        "Max liczba artyku≈Ç√≥w",
        min_value=5,
        max_value=5000,
        value=200,
        step=5,
        help="Limit bezpiecze≈Ñstwa. Przy du≈ºych serwisach zacznij od 100‚Äì300.",
    )

with colC:
    similarity_threshold = st.slider(
        "Pr√≥g podobie≈Ñstwa (%)",
        min_value=50,
        max_value=95,
        value=70,
        step=1,
        help="Pary powy≈ºej progu traktuj jako potencjalnƒÖ kanibalizacjƒô.",
    )

advanced = st.expander("Ustawienia zaawansowane", expanded=False)
with advanced:
    c1, c2, c3, c4 = st.columns(4)
    with c1:
        request_timeout = st.number_input("Timeout request√≥w (s)", 5, 60, 20, 1)
    with c2:
        delay_seconds = st.number_input("Op√≥≈∫nienie miƒôdzy requestami (s)", 0.0, 5.0, 0.6, 0.1)
    with c3:
        crawl_depth = st.number_input("Maks. g≈Çƒôboko≈õƒá crawl", 1, 8, 3, 1)
    with c4:
        same_subdomain_only = st.checkbox(
            "Tylko ten sam subdomen",
            value=True,
            help="Je≈õli odznaczysz, zbierze linki z ca≈Çej domeny g≈Ç√≥wnej.",
        )

    ua = st.text_input(
        "User-Agent",
        value="Mozilla/5.0 (compatible; SEOContentAuditor/1.0; +https://github.com/your-repo)",
    )

run = st.button("üöÄ Uruchom scrapowanie i analizƒô", type="primary")

if "articles_df" not in st.session_state:
    st.session_state["articles_df"] = None
if "report_df" not in st.session_state:
    st.session_state["report_df"] = None
if "sim_matrix" not in st.session_state:
    st.session_state["sim_matrix"] = None

if run:
    if not base_url or not base_url.startswith(("http://", "https://")):
        st.error("Podaj poprawny URL zaczynajƒÖcy siƒô od http:// lub https://")
        st.stop()

    status = st.status("Startujƒô‚Ä¶", expanded=True)
    progress = st.progress(0)

    def on_progress(done: int, total: int, message: str):
        if total > 0:
            progress.progress(min(1.0, done / total))
        status.write(message)

    try:
        status.update(label="üîé Wykrywam i pobieram artyku≈Çy‚Ä¶", state="running")
        articles_df = scrape_site_articles(
            base_url=base_url,
            max_pages=int(max_pages),
            timeout=int(request_timeout),
            delay=float(delay_seconds),
            max_depth=int(crawl_depth),
            user_agent=ua,
            same_subdomain_only=bool(same_subdomain_only),
            progress_callback=on_progress,
        )

        if articles_df.empty:
            status.update(label="Nie znaleziono artyku≈Ç√≥w lub nie uda≈Ço siƒô pobraƒá tre≈õci.", state="error")
            st.stop()

        st.session_state["articles_df"] = articles_df

        status.update(label="üß† Liczƒô podobie≈Ñstwa‚Ä¶", state="running")
        # Macierz (do wizualizacji) + raport par powy≈ºej progu
        sim_matrix = build_similarity_matrix(articles_df["tre≈õƒá w Markdown"].fillna("").tolist())
        report_df = compute_similarity_report(
            articles_df=articles_df,
            sim_matrix=sim_matrix,
            threshold=float(similarity_threshold) / 100.0,
        )

        st.session_state["report_df"] = report_df
        st.session_state["sim_matrix"] = sim_matrix

        status.update(label="‚úÖ Gotowe", state="complete")
        progress.progress(1.0)

    except Exception as e:
        status.update(label="‚ùå B≈ÇƒÖd", state="error")
        st.exception(e)

articles_df = st.session_state.get("articles_df")
report_df = st.session_state.get("report_df")
sim_matrix = st.session_state.get("sim_matrix")

if articles_df is not None:
    st.subheader("1) Dane artyku≈Ç√≥w (CSV: URL, H1, title, tre≈õƒá w Markdown)")
    st.dataframe(articles_df, use_container_width=True, height=350)

    csv_bytes = articles_df.to_csv(index=False).encode("utf-8")
    st.download_button(
        "‚¨áÔ∏è Pobierz CSV z artyku≈Çami",
        data=csv_bytes,
        file_name="articles_markdown.csv",
        mime="text/csv",
    )

if report_df is not None:
    st.subheader("2) Potencjalna kanibalizacja (pary powy≈ºej progu)")
    if report_df.empty:
        st.info("Brak par powy≈ºej ustawionego progu.")
    else:
        st.dataframe(report_df, use_container_width=True, height=350)
        rep_bytes = report_df.to_csv(index=False).encode("utf-8")
        st.download_button(
            "‚¨áÔ∏è Pobierz raport kanibalizacji (CSV)",
            data=rep_bytes,
            file_name="cannibalization_report.csv",
            mime="text/csv",
        )

if sim_matrix is not None and articles_df is not None:
    st.subheader("3) Wizualizacja podobie≈Ñstwa (heatmap ‚Äì top 60 URLi)")
    st.caption("Dla czytelno≈õci pokazujƒô maks. 60 pierwszych artyku≈Ç√≥w (mo≈ºesz przefiltrowaƒá listƒô w kodzie).")

    import matplotlib.pyplot as plt

    n = min(60, sim_matrix.shape[0])
    m = sim_matrix[:n, :n]
    labels = [f"{i+1}" for i in range(n)]

    fig = plt.figure()
    plt.imshow(m, interpolation="nearest")
    plt.colorbar()
    plt.xticks(range(n), labels, rotation=90, fontsize=7)
    plt.yticks(range(n), labels, fontsize=7)
    plt.title("Cosine similarity (TF-IDF) ‚Äì skr√≥cona macierz")
    plt.tight_layout()
    st.pyplot(fig)

    with st.expander("Mapa indeks√≥w ‚Üí URL", expanded=False):
        tmp = articles_df[["URL", "title"]].head(n).copy()
        tmp.insert(0, "idx", np.arange(1, n + 1))
        st.dataframe(tmp, use_container_width=True, height=300)
